{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_clip = pd.read_csv('user_clip.csv').dropna()\n",
    "r_avg = user_clip['weight'].mean()\n",
    "users_bias = user_clip.groupby('user_id')['weight'].mean() - r_avg\n",
    "clips_bias = user_clip.groupby('clip_id')['weight'].mean() - r_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv').filter(['user_id', 'clip_id']).dropna()\n",
    "users_bias = users_bias.reset_index().rename(columns={'index': 'user_id', 'weight': 'user_bias'})\n",
    "clips_bias = clips_bias.reset_index().rename(columns={'index': 'clip_id', 'weight': 'clip_bias'})\n",
    "test_df = test_df.merge(users_bias, on=['user_id'], how='left')\n",
    "test_df = test_df.merge(clips_bias, on=['clip_id'], how='left')\n",
    "test_df['prediction'] = r_avg + test_df['user_bias'] + test_df['clip_bias']\n",
    "test_df.filter(['user_id', 'clip_id', 'prediction']).rename(columns={'prediction': 'weight'}).to_csv('319044434_314779166_task1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clip = user_clip.merge(users_bias, on=['user_id'], how='left')\n",
    "user_clip = user_clip.merge(clips_bias, on=['clip_id'], how='left')\n",
    "user_clip['prediction'] = r_avg + user_clip['user_bias'] + user_clip['clip_bias']\n",
    "user_clip['prediction'] = user_clip['prediction'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965092157923.618\n"
     ]
    }
   ],
   "source": [
    "def f1(df):\n",
    "    error = ((df['prediction'] - df['weight']) ** 2).sum()\n",
    "    regularization = 0.1 * ((df['user_bias'] ** 2).sum() + (df['clip_bias'] ** 2).sum())\n",
    "    return error + regularization\n",
    "\n",
    "print(f1(user_clip))\n",
    "\n",
    "# TODO: Question for the metargel:\n",
    "# 1. What do we do with predictions below 0 (both in train and test)?\n",
    "# 2. What do we do with missing values (currently - we drop them)?\n",
    "# 3. Is f1 on the test or train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
